\section*{Miscellaneous}

\begin{defn}
    $\Gamma(\alpha)$ denotes the gamma function which is defined as $\Gamma(\alpha) = \displaystyle\int_0^\infty t ^{\alpha - 1} e ^{-t} \mathop{\mathrm{d} t}$, where $\alpha > 0$. It is a commonly used extension of the factorial function.
\end{defn}

\begin{thm}
    The Gamma function has the following properties:
    \begin{enumerate}
        \item The improper integral converges for all $\alpha > 0$.
        \item $\Gamma(\alpha) > 0$  for all $\alpha > 0$, $\Gamma(\alpha) \to \infty$ as $\alpha \to 0$.
        \item $\Gamma(1) = 1$
        \item $\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)$
        \item For $n \in \mathbb{N}$, $\Gamma(n) = (n-1)!$
        \item $\Gamma(\frac{1}{2}) = \sqrt{\pi}$
    \end{enumerate}
\end{thm}

\begin{defn}
    For $\alpha > 0, \beta > 0$, the beta function is defined as $B(\alpha, \beta) = \displaystyle\int_0^1 x^{\alpha -1} (1-x)^{\beta - 1} dx$.
    \\

    Alternatively, the beta function can be written as $B(\alpha, \beta) = \displaystyle\int_0^\infty \frac{t^{\alpha - 1}}{(1+t)^{\alpha + \beta}} dt$
\end{defn}

\begin{prop}[Relation between beta function and gamma function]\ \\
    $B(\alpha, \beta) = \displaystyle\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}$
\end{prop}


\begin{prop}[Limit Comparison Test for Improper Integrals]
    Let $a \in \mathbb{R}$ and $f,g: [a, \infty) \to \mathbb{R}$ be such that both $f$ and $g$ are integrable on $[a,x]$ for every $x \geq a$ with $f(t) > 0$ and $g(t) > 0$ for all large $t$. \\ Assume that
    $\displaystyle \lim_{t \to \infty} \frac{f(t)}{g(t)} = l$ where $l \in [0, \infty]$. Then:
    \begin{itemize}
        \item If $l \in (0, \infty)$, then $\int_a^\infty f(x)dx$ converges $\iff$ $\int_a^\infty g(x)dx$ converges
        \item If $l = 0$, and $\int_a^\infty g(x)dx$ converges then $\int_a^\infty f(x)dx$ converges
        \item If $l = \infty$ and $\int_a^\infty f(x)dx$ converges then $\int_a^\infty g(x)dx$ converges absolutely
    \end{itemize}
\end{prop}

\section*{Fundamentals}

\begin{defn}
    Two random vectors $(X_1, X_2, ... , X_n)$ and $(Y_1, Y_2, ... , Y_n)$ are said to be \textbf{independent} if $F(x_1, x_2, ..., x_m, y_1, y_2, ..., y_n) = F_1 (x_1, x_2, ..., x_m) F_2 (x_1, x_2, ..., x_n)$ for all $(x_1, x_2, ..., x_m, y_1, y_2, ..., y_n) \in \mathbb{R}^{m+n}$ where $F, F_1, F_2$ are the joint CDF's of $(X_1, X_2, ..., X_m, Y_1, Y_2, ..., Y_n)$, $(X_1, X_2, ..., X_m)$ and $(Y_1, Y_2, ..., Y_n)$ respectively.
\end{defn}

\begin{thm}
    Let $X = (X_1, X_2, ... , X_m)$ and $Y = (Y_1, Y_2, ... , Y_n)$ be independent random vectors. Then the component $X_j$ of $X(j = 1, 2, ... , m)$ and the component $Y_k$ of $Y(k = 1, 2, ... , n)$ are independent random variables. If $h$ and $g$ are Borel-measurable functions, $h(X_1, X_2, ..., X_m)$ and $g(Y_1, Y_2, ... , Y_n)$ are independent.
\end{thm}

\begin{thm}
    Suppose $(X,Y)$ have joint pdf $f$. Then $X$ and $Y$ are independent iff for some constant $k > 0$ and non-negative functions $f_1$ and $f_2$, $f(x,y) = k f_1(x)f_2(y)$ for all $(x,y) \in \mathbb{R}^2$
\end{thm}

\begin{thm}
    Let $X$ be a random variable with pdf $f(x)$. Then the pdf of $aX+b$ where $a \neq 0, b \in \mathbb{R}$ is given by $\displaystyle \frac{1}{a} f \left (\frac{x-b}{a} \right)$
\end{thm}

\begin{thm}
    If $X$ and $Y$ are independent continuous random variables with pdfs $f_X(x)$ and $f_Y(y)$, then the pdf of $Z = X+Y$ is $f_Z\left (z\right ) = \displaystyle\int_{- \infty}^\infty f_X \left (w\right ) f_Y \left (z - w\right ) \mathop{\mathrm{d} w}$
\end{thm}

\begin{prop}
    Let $(X,Y)$ be a random vector with joint density $f(x,y)$ and $g,h$ be continuous and differentiable real valued functions of two variables. Then to obtain the joint pdf of $(g(X,Y), h(X,Y))$ we consider the equations $g(x,y) = z$ and $h(x,y) = w$. There may be many such points $(x,y)$ which map to $z$ and $w$ under $g$ and $h$ respectively.

    Let the points $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$ represent the points which satisfy $g(x_i, y_i) = z$ and $h(x_i, y_i) = w$.

    We can find these points as $\{x_i\} = g^{-1}(z,w)$ and $\{ y_i \} = h^{-1}(z,w)$.

    Compute $J(x_i, y_i) = \begin{bmatrix} \frac{\partial g}{\partial x} & \frac{\partial g}{\partial y} \\ \frac{\partial h}{\partial x} & \frac{\partial h}{\partial y} \end{bmatrix}_{(x = x_i, y = y_i)}$

        Then the joint pdf is $k(z,w) = \displaystyle \sum_i \frac{1}{|J(x_i, y_i)|} f(x_i, y_i)$
\end{prop}

\subsection*{Expectation, Variance and Moments}

\begin{defn}
    The \textbf{expectation} of a discrete random variable $X$ having values $x_1, x_2, ..., x_n$ and probability function $f(x)$ is defined as $E\left (X\right ) = \sum_{i=1}^n x_i f\left (x_i\right )$.
    \\

    If $X$ is a discrete random variable taking on infinite set of values $x_1, x_2, ...$, then $E(X) = \sum_{i=1}^\infty x_i f(x_i)$ provided the infinite series converges absolutely.
    \\

    For a continuous random variable $X$ with distribution function $f(x)$, the expectation of $X$ is defined as $E(X) = \int_{- \infty}^\infty x f(x) dx$ provided the integral converges absolutely.
\end{defn}


\begin{samepage}
\begin{thm}
    The expectation has the following properties:

    \begin{enumerate}
        \item $E(c X) = c E(X)$ where $c$ is any constant
        \item If $X$ and $Y$ are any random variables then $E(X + Y) = E(X) + E(Y)$
        \item If $X$ and $Y$ are independent random variables, then $E(XY) = E(X)E(Y)$
    \end{enumerate}
\end{thm}
\end{samepage}

\begin{defn}
    The \textbf{variance} is defined as $\text{Var} (X) = \sigma ^2 = E[ \  (X - \mu)^2 \  ]$.
    The \textbf{standard deviation} is defined as $\sigma = \sqrt{\text{Var} (X)}$.
\end{defn}

\begin{samepage}
\begin{thm}
    The variance has the following properties:

    \begin{enumerate}
        \item $\sigma ^2 = E\left [ \left (X - \mu \right ) ^2 \right ] = E\left (X ^2\right ) - \mu ^2 = E\left (X ^2\right ) - \left [E\left (X\right )\right ] ^2$ where $\mu = E\left (X\right )$
        \item If $c$ is any constant, $\text{Var}(cX) = c^2 \text{Var}(X)$
        \item The quantity $E[ (X - a)^2 ]$ is a minimum where $a = \mu = E(X)$
        \item If $X$ and $Y$ are independent random variables, then \\ $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)$ and $\text{Var}(X-Y) = \text{Var}(X) + \text{Var}(Y)$
    \end{enumerate}
\end{thm}
\end{samepage}

\begin{defn}
    The $r$-th \textbf{moment} of a random variable $X$ about the mean $\mu$ is defined as $\mu_r = E[\  (X - \mu)^r \ ]$ where $r=0, 1, 2, ...$.

    The $r$-th moment of $X$ about the origin is defined as $\mu_r ' = E(X^r)$.
\end{defn}

It follows that $\mu_0 = 0$, $\mu_1 = 1$, $\mu_2 = \sigma ^2$

\begin{samepage}
\begin{thm}[Law of the unconscious statistician - LOTUS]
    Let $X$ be a discrete random variable with probability function $f(x)$. Then $Y = g(x)$ is also a discrete random variable. \\ The probability function of $Y$ is $h(y) = P(Y = y) = \sum_{ \{x | g(x) = y \} } P(X = x) = \sum_{ \{x | g(x) = y \} } f(x)$.

    If $X$ takes on values $x_1, x_2, ..., x_n$  and $Y$ takes on values $y_1, y_2, ..., y_m$, then $m \leq n$ and $y_1 h(y_1) + y_2 h(y_2) + ... + y_m h(y_m) = g(x_1) f(x_1) + g(x_2) f(x_2) + ... + g(x_n) f(x_n)$ which lets us write the expectation of $Y$ as \\ $E(Y) = g(x_1)f(x_1) + g(x_2)f(x_2) + ... + g(x_n)f(x_n) = \sum_{i=1}^n g(x_i)f(x_i)$.
    \\ 

    Similarly when $X$ is a continuous random variable and $Y=g(X)$, then $E(Y) = \int_{- \infty}^{\infty} g(x) f(x) dx$.
\end{thm}
\end{samepage}

\begin{defn}
    Let $X$ be a random variable defined on $\left (\Omega,  \mathcal{F}, P\right )$. The function $M\left (t\right ) = E\left [e ^{t X}\right ]$ is called the \textbf{moment generating function} (MGF) of the random variable $X$ if the expectation on the right side exists in some neighbourhood of the origin. If the expectation on the right side does not exist in any neighbourhood of the origin, then we say the MGF does not exist.
\end{defn}

\begin{samepage}
The $r$-th derivative of the moment generating funtion is the $r$-th moment about the origin $\mu_r '$.
\begin{thm}
    If the MGF $M(s)$ of a random variable $X$ exists, then the MGF $M(s)$ has derivatives of all orders at $s=0$ and \\

    $M^{(k)} (s) |_{s=0} = EX^k$ for positive integer $k$
\end{thm}

\end{samepage}
\begin{thm}
    The moment generating function has the following properties:
    \begin{enumerate}
        \item For any constants $a$ and $b$, the mgf of the random variable $a X + b$ is given by $M_{aX+b} = e^{bt} M_X(at)$
        \item If $X$ and $Y$ are independent random variables having moment generating functions $M_X(t)$ and $M_Y(t)$ respectively, then $M_{X+Y}(t) = M_X(t) M_Y(t)$
        \item \textbf{Uniqueness Theorem} Suppose that $X$ and $Y$ are random variables having moment generating functions $M_X(t)$ and $M_Y(t)$ respectively. Then $X$ and $Y$ have the same probability distribution if and only if $M_X(t) = M_Y(t)$ identically.
    \end{enumerate}
\end{thm}

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a jointly distributed or $(X_1, X_2, ..., X_n)$ be a random vector. If $E[\exp ( \sum_{j=1}^n t_j X_j )]$ exists for $|t_j| \leq h_j$, $j = 1, 2, ...,n$, we write $M(t_1, t_2, ..., t_n) = E[\exp(t_1 X_1 + t_2 X_2 + ... + t_n X_n)]$ and call it the MGF of $X_1, X_2, ..., X_n$ or simply, the \textbf{joint moment generating function} (joint MGF) of the random vector $(X_1, X_2, ..., X_n)$
\end{defn}

\begin{thm}
    The joint MGF $M(t_1, t_2)$ uniquely determines the joint distribution of $(X, Y)$.
    Conversely, if the joint MGF exists it is unique.
\end{thm}

\begin{thm}
    The joint MGF $M(t_1, t_2)$ completely determines the marginal distributions of $X$ and $Y$.

    $M(t_1, 0) = E[\exp(t_1 X)] = M_X (t_1)$ and $M(0, t_2) = E[\exp(t_2 X)] = M_Y(t_2)$
\end{thm}

\begin{thm}
    $X$ and $Y$ are independent random variables if and only if $M(t_1, t_2) = M(t_1, 0) M(0, t_2)$ for all $t_1 \in [-h_1, h_1], t_2 \in [-h_2, h_2]$
\end{thm}


\subsection*{Distributions}

\begin{defn}
    Let $p$ be the probability that an event will happen in any single Bernoulli trial (trial with outcomes either success or failure). The probability that an event will happen exactly $x$ times in $n$ trials is given by the \textbf{Binomial Random Variable} with pmf distribution $f(x) = {n \choose k} p^x (1-p)^{n-x} = \displaystyle \frac{n!}{x! (n-x)!} p^x (1-p)^{n-x}$, where $x = 0, 1, ... , n$.
\end{defn}

\begin{prop}
The binomial random variable has mean $\mu = np$ and variance $\sigma^2 = npq$.
\end{prop}

\begin{defn}
    The \textbf{Poisson Random Variable} has pmf distribution $f(x) = \displaystyle \frac{\lambda^x e^{-\lambda}}{x!}$, $x = 0, 1, 2, ...$ and $\lambda > 0$.
\end{defn}

\begin{prop}
The Poisson Random Variable has mean $\mu = \lambda$ and variance $\sigma^2 = \lambda$.
\end{prop}
\begin{prop}
    The Poisson Random Variable with $\lambda = np$ is the limiting case of the Binomial Distribution. It approximates the Binomial Random variable $\text{Binomial}(n,p)$ when $n$ is large and probability of occurrence of an event $p$ is close to $0$.
\end{prop}

\begin{defn}    
    The \textbf{uniform random variable} has the pdf distribution $f(x) = \begin{cases} \frac{1}{b-a} & a \leq x \leq b \\ 0 & \text { otherwise }\end{cases}$.
\end{defn}

\begin{prop}
The uniform random variable has mean $\mu = \frac{a + b}{2}$ and variance $\sigma^2 = \frac{1}{12} (b-a)^2$.
\end{prop}

\begin{defn}
    The \textbf{exponential random variable} has the pdf distribution $f(x) = \begin{cases} \lambda e^{-\lambda x} & x > 0 \\ 0 & \text{ otherwise } \end{cases}$
\end{defn}

\begin{prop}
The exponential random variable has mean $\frac{1}{\lambda}$ and variance $\frac{1}{\lambda^2}$.
\end{prop}

\begin{defn}
    The \textbf{normal random variable}, also known as the gaussian random variable, has pdf distribution $f(x) = \displaystyle \frac{1}{\sigma \sqrt{2 \pi}}  \exp\left(\displaystyle - \frac{(x-\mu)^2}{2 \sigma^2}\right)$ where $-\infty < x < \infty$ where $\mu$ and $\sigma$ are the mean and standard deviation respectively.
\end{defn}

\begin{defn}
When $\mu = 0$ and $\sigma = 1$, we get the \textbf{standard normal random variable} with distribution $f(x) = \displaystyle\frac{1}{\sqrt{2\pi}} \exp(-\frac{x^2}{2})$.
\end{defn}

We can write any normal random variable $Y \sim N(\mu, \sigma)$ in terms of the standard normal random variable $X \sim N(0,1)$ as $Y = \sigma X + \mu$.

\begin{prop}
    When $n$ is large and neither $p$ or $q$ is too close to $0$, the binomial random variable $X$ can be approximated by a normal distribution with mean $np$ and standard deviation $\sqrt{npq}$. The approximation is very good when $np, nq > 5$.
\end{prop}

\begin{prop}
    The Poisson Distribution approaches the normal distribution $N(\lambda, \sqrt{\lambda})$ as $\lambda \to \infty$, i.e. the Poisson distribution is asymptotically normal.
\end{prop}

\begin{defn}
    A variable the pdf $f(x) = \displaystyle \frac{1}{\sigma \pi (1 + \left( \frac{x - \mu}{\sigma} \right)^2 )}$, $x \in \mathbb{R}$ where $\sigma > 0, \mu \in \mathbb{R}$ is called a \textbf{Cauchy random variable} with parameter $\mu$ and $\sigma^2$.
    We write $X \sim \mathcal{C}(\mu, \sigma^2)$ for Cauchy random $X$ with pdf.
\end{defn}

\begin{defn}
    When $\mu = 0$ and $\sigma^2 = 1$, we get the \textbf{standard Cauchy random variable} $C(0,1)$ with distribution $f(x) = \displaystyle \frac{1}{\pi (1 + x^2)}$.
\end{defn}

We can write any Cauchy random variable $Y = C(\mu, \sigma^2)$ in terms of the standard Cauchy random variable $X = C(0,1)$ as $Y = \sigma X + \mu$.

\begin{prop}
The mean, variance, higher moments, moment generating function of a Cauchy random variable do not exist.
\end{prop}

\begin{defn}
    A random variable with the pdf $f(x) = \displaystyle  \frac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha - 1} e^{-x/\beta}$ where $0 < x < \infty, \alpha > 0, \beta > 0$ is called a \textbf{gamma random variable} $G(\alpha, \beta)$ with parameters $\alpha$ and $\beta$.
\end{defn}
\begin{prop}
    When $\alpha=1$, we see that the gamma distribution is a generalization of the exponential distribution as \\
$G(1, \beta) = \text{exp}(\beta)$ with pdf $f(x) = \frac{1}{\beta} e^{-x/\beta}$, $x > 0$.
\end{prop}

\begin{prop}
The gamma distribution has mean $\mu = \alpha \beta$ and variance $\sigma^2 = \alpha \beta^2$.
\end{prop}

\begin{defn}
    The \textbf{chi-square distribution} is \\
    $f(x) = \displaystyle \frac{1}{\Gamma(\frac{p}{2}) 2^{\frac{p}{2}}} x^{\frac{p}{2} - 1} e ^{-x/2}$ where $0 < x < \infty$.

    \vspace{0.5cm}
    $\chi_p^2$ denotes a chi-square random variable with $p$ degrees of freedom.
\end{defn}

The chi-square distribution is a special case of the gamma distribution with $\alpha = \frac{p}{2}$ and $\beta = 2$.

\begin{prop}
The mean of the chi-square distribution is given by $\mu = p$ and the variance is given by $\sigma^2 = 2p$.
\end{prop}

\begin{prop}
Let $X \sim N(0,1)$. Then $X^2 \sim \chi_1^2$
\end{prop}

\begin{prop}
    Let $X_1, X_2, ..., X_n$ be independent normal random variables with mean $0$ and variance $1$. Then $\chi^2 = X_1^2 + X_2^2 + ... + X_p^2$ is chi-square distributed with $p$ degrees of freedom.
\end{prop}

\begin{defn}
    A random variable $T$ has the \textbf{Students t-distribution} with $p$ degrees of freedom, and we write $T \sim t_p$ if it has pdf $f_p (t) = \displaystyle \frac{\Gamma(\frac{p+1}{2})}{\Gamma(\frac{p}{2})} \frac{1}{(p \pi)^{\frac{1}{2}}} \frac{1}{(1 + \frac{t^2}{p})^{\frac{(p+1)}{2}}} $ for $- \infty < t < \infty$
\end{defn}

\begin{prop}
    If $p=1$, $T$ is a Cauchy$(0,1)$ distribution with distribution $f_p(t) = \displaystyle\frac{\Gamma(1)}{\Gamma(\frac{1}{2})} \frac{1}{\pi^\frac{1}{2}} \frac{1}{1+t^2}$. So we will assume that $p > 1$.
\end{prop}

\begin{prop}
    Let $T \sim t_p$. Then $E[T^r]$ exists for $r < p$ and \\ $E[T^r] = \begin{cases} 0 & \text{ if } r \text{ is odd } \\   \displaystyle p^{\frac{r}{2}} \frac{\Gamma(\frac{r+1}{2})\Gamma(\frac{p-r}{2})}{\Gamma(\frac{p}{2})\Gamma(\frac{1}{2})} & \text{ if } r \text{ is even }\end{cases}$
\end{prop}

\begin{prop}
    The Students t-distribution has no MGF because it does not have moments of all orders.
\end{prop}

\begin{prop}
    Let $T \sim t_p$, $p > 2$ be a random variable with Student's $t$-distribution. Then $T$ has mean $\mu = 0$ and variance $\sigma^2 = \displaystyle \frac{p}{p-2}$.
\end{prop}

\begin{defn}
    A random variable $X \sim F(m,n)$ has the \textbf{$F$-distribution} with $m$ and $n$ degrees of freedom if it has pdf \\ 
    $ f_F(t) = \displaystyle \frac{\Gamma\left(\frac{m + n}{2} \right)}{\Gamma\left(\frac{m}{2} \right) \Gamma\left(\frac{n}{2} \right)} \displaystyle \left(\frac{m}{n}\right)^{\frac{m}{2}} \frac{t^{\frac{m}{2} - 1}}{(1 + \frac{m}{n}t)^{\frac{m + n}{2}}} $ where $t > 0$
\end{defn}

\begin{prop}
    If $X \sim t_n$, then $X^2 \sim F(1,n)$. In particular if $X \sim C(0,1)$, i.e. $X \sim t_1$, then $X^2 \sim F(1,1)$.
\end{prop}

\begin{prop}
    Let $X \sim F(m,n)$. Then for $k \in \mathbb{N}$, $E[X^k] =\displaystyle \left(\frac{n}{m}\right)^k \frac{\Gamma(k + \frac{m}{2}) \Gamma(\frac{n}{2} - k)}{\Gamma(\frac{m}{2}) \Gamma(\frac{n}{2})}$ for $n > 2k$.
\end{prop}

\begin{prop}
    Let $X \sim F(m,n)$. Then $X$ has mean $\mu = \displaystyle \frac{n}{n-2}$ and variance $\sigma^2 = \displaystyle \frac{n^2 (2m + 2n - 4)}{m (n-2)^2 (n-4)}$ for $n > 4$.
\end{prop}

\begin{defn}
    A random variable $X \sim \text{beta}(\alpha, \beta)$ has the \textbf{beta distribution} if it has pdf \\
    $ f(x) = \displaystyle \frac{1}{B(\alpha, \beta)} x^{\alpha-1} (1-x)^{\beta - 1}$ for $0 < x < 1$.
\end{defn}

\begin{defn}
    A random variable $X$ has the \textbf{Pareto distribution} with parameters $\alpha > 0$ and $\beta > 0$ if it has pdf \\
    $f(x) = \begin{cases} \displaystyle \frac{\beta \alpha^\beta}{x^{\beta + 1}} & x \geq \alpha \\ 0 & x \leq \alpha \end{cases}$
\end{defn}

\begin{prop}
    For Pareto's distribution with parameter $\alpha$ and $\beta$, the moment of order $n$ exists if and only if $n < \beta$. 
\end{prop}
