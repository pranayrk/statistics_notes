\section*{Sampling Theory}
We can either sample \textit{with replacement} or \textit{without replacement}. A finite population sampled with replacement can be considered infinite. Sampling from a very large finite population can similarly be considered as sampling from an infinite population. 

To properly choose the sample, we can make sure that every member of the population has an equal chance of being in the sample.
Normally, since the sample size is much smaller than the population size, sampling without replacement will give practically the same results as sampling with replacement.

For a sample of size $n$ from a population which we assume has distribution $f(x)$, we can choose members of the population at random, each selection corresponding to a random variable $X_1, X_2, ..., X_n$ with corresponding values $x_1, x_2, ..., x_n$. In case we are assuming sampling without replacement, $X_1, X_2, ..., X_n$ will be independent and identically distributed random variables with probability distribution $f(x)$.

\begin{defn}
Let $X$ be a random variable with a distribution $f$, and let $X_1, X_2, ..., X_n$ be iid random variables with the common distribution $f$.

Then the collection $X_1, X_2, ..., X_n$ is called a \textbf{random sample} of size $n$ from the population $f$.
\end{defn}

Since $X_1, X_2, ..., X_n$ are iid, the joint distribution of the random sample is \\
$f(x_1, x_2, ..., x_n) = f(x_1) f(x_2) ... f(x_n)$.

Any quantity obtained from a sample for the purpose of estimating a population parameter is called a sample
statistic, or briefly statistic. Mathematically, a sample statistic for a sample of size n can be defined as a function of the random variables $X_1, X_2, ..., X_n$ as $T(X_1, X_2, ..., X_n)$. This itself is a random variable whose values can be represented as $T(x_1, x_2, ..., x_n)$.

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a random sample of size $n$ from the population whose distribution is $f\left (x|\theta\right )$ (the distribution $f$ with unknown parameter $\theta$). Let $T\left (x_1, x_2, ...,x_n\right )$ be a real-valued or vector-valued function whose domain includes the range of $\left (X_1, X_2, ..., X_n\right )$. Then the random variable or random vector $Y = T\left (X_1, ..., X_n\right )$ is called a \textbf{statistic} provided that $T$ is not a function of any unknown parameter $\theta$.
\end{defn}

For example consider $X \approx N(\mu, \sigma^2)$ where $\mu$ is known but $\sigma$ is unknown. Then $\frac{\sum_{i=1}^n X_i}{\sigma^2}$ is not a statistic but $\frac{\sum_{i=1}^n X_i}{\mu^2}$ is a statistic.

Two common statistics are the sample mean and sample variance.

\begin{defn}
    The \textbf{sample mean} is the arithmetic average of the values in the random sample. It is denoted by $\bar{X} = \displaystyle \frac{X_1 + X_2 + ... + X_n}{n}$.

    The \textbf{sample variance} is the statistic defined by $S^2 = \displaystyle \frac{\sum_{i=1}^n \left (X_i - \bar{X}\right )^2}{n-1}$

    The \textbf{sample standard deviation} is the statistic defined by $S = \sqrt{S^2}$.
\end{defn}

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a random sample from a population $f\left (x| \theta\right )$. We say that a statistic $T\left (X_1, X_2, ...,X_n\right )$ is an \textbf{unbiased estimator} of the parameter $\theta$ if $E(T) = \theta$ for all possible values of $\theta$.
\end{defn}

\begin{thm}
    Let $X_1, X_2, ..., X_n$ be a random sample from a population with mean $\mu $ and variance $\sigma ^2 < \infty$. Then:
    \begin{enumerate}
        \item $E\left (\bar{X}\right ) = \mu $
        \item $\text{Var}\left (\bar{X}\right ) = \frac{\sigma ^2}{n}$
        \item $E\left (S ^2\right ) = \sigma ^2$
    \end{enumerate}
\end{thm}

From the above theorem we see that the sample mean $\bar{X}$ is an unbiased estimator of the population mean $\mu$ and the sample variance $S^2$ is an unbiased estimator of the population variance $\sigma^2$. (The reason we included $\frac{1}{n-1}$ in the  definition of the sample variance was to make it an unbiased estimator)

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a random sample of size $n$ from a population $f(x|\theta)$. The probability distribution of a statistic $T(X_1, X_2, ..., X_n)$ is called the sampling distribution of $T$.
\end{defn}

\begin{thm}[MGF of the sample mean]
    Let $X_1, X_2, ..., X_n$ be a random sample from a population with MGF $M_X(t)$. Then the MGF of the sample mean is $M_{\bar{X}} (t) = (M_X (t/n))^n$.
\end{thm}

\begin{thm}
    Let $X_1, X_2, . . . , X_n$ be a random sample from a $N(\mu, \sigma^2)$ distribution, and let $X$ denote the sample mean.

    Then $X$ and the random vector $(X_1 - X, X_2 - X, . . . , X_n - X)$ are independent.
\end{thm}

\begin{thm}
    Let $X_1, X_2, . . . , X_n$ be a random sample from a $N(\mu, \sigma^2)$ distribution, 
    and let $X$ denote the sample mean and $S^2$ denote the sample variance. Then $X$ and $S^2$ are independent random variables.
\end{thm}
The converse of this theorem is also true: if the sample mean and sample variance of a random sample are independent random variables then population distribution is normal.

\begin{thm}
    Let $X_1, X_2, ... , X_n$ be a random sample from a $N(\mu, \sigma^2)$ distribution and let $\overline{X}$ denote the sample mean and $S^2$ denote the sample variance. Then $(n-1) \frac{S^2}{\sigma^2}$ has a chi-square distribution with $(n-1)$ degrees of freedom.
\end{thm}

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a random sample and $x_1, x_2, ..., x_n$ be values taken by these random variables.
    Arrange $(x_1, x_2, ..., x_n)$ in increasing order of magnitude so, $x_{(1)} \leq x_{(2)} \leq ... \leq x_{(n)}$
    where $x_{(1)} = \text{min}(x_1, x_2, ..., x_n)$, $x_{(2)}$ is the second smallest value and so on and $x_{(n)} = \text{max}(x_1, x_2, ..., x_n)$. If any two $x_i, x_j$ are equal, their order does not matter.

    
    The function $X_{(k)}$ of $(X_1, X_2, ..., X_n)$ that takes on the value $x_{(k)}$ in each possible sequence $(x_1, x_2, ... , x_n)$ of values assumed by $(X_1, X_2, ..., X_n)$ is known as the \textbf{$k$-th order statistic}. 

    ${X_{(1)}, X_{(2)}, ..., X_{(n)}}$ is called the \textbf{set of order statistics} for $(X_1, X_2, ..., X_n)$
\end{defn}

\begin{defn}
    The \textbf{sample range} $R = X_{(n)} - X_{(1)}$ is the distance between the smallest and largest observations. It is the measure of the dispersion in the sample and should reflect the dispersion of the population.
\end{defn}

\begin{defn}
    In terms of order statistics, the \textbf{sample median} $M$ is defined by \\ 
    $M = \begin{cases} X_{(\frac{n+1}{2})} & \text{ if } n \text{ is odd } \\ \frac{1}{2} \left[ X_{(\frac{n}{2})} + X_{(\frac{n}{2} + 1)}\right] & \text{ if } n \text{ is even } \end{cases}$.
\end{defn}

\begin{thm}
    Let $X_{(1)}, X_{(2)}, ..., X_{(n)}$ denote the order statistics of the random sample $X_1, X_2, ..., X_n$ from a continuous population with cdf $F_X(x)$ and the pdf $f_X(x)$.
    Then the pdf of $X_{(j)}$ is 
    \\$f_{X_{(j)}}(x)= \displaystyle \frac{n!}{(j-1)! (n-j)!} f_X(x) [F_X(x)]^{j-1} [1-F_X(x)]^{n-j}$ for $x \in \mathbb{R}$
\end{thm}

\begin{thm}
    Let $X_{(1)}, X_{(2)}, ..., X_{(n)}$ denote the order statistics of the random sample $X_1, X_2, ..., X_n$ from a continuous population with cdf $F_X(x)$ and the pdf $f_X(x)$. Then the joint pdf of all the order statistics is given by
    \\$ f_{X_{(1)}, X_{(2)}, ..., X_{(n)}} = \begin{cases} n! f_X(x_1) ... f_X(x_n) & -\infty < x_1 < x_2 < ... < x_n < \infty \\ 0 & \text{ otherwise }\end{cases}$
\end{thm}

\begin{thm}
    Let $X_{(1)}, X_{(2)}, ..., X_{(n)}$ denote the order statistics of the random sample $X_1, X_2, ..., X_n$ from a continuous population with cdf $F_X(x)$ and the pdf $f_X(x)$.

    Then the joint pdf of $X_{(i)}$ and $X_{(j)}$ where $1 \leq i \leq j \leq n$ is given by \\ 
    $ f_{X_{(i)}, X_{(j)}}(u,v) = \begin{cases} \displaystyle \frac{n! f_X(u) f_X(v) [F_X(u)]^{i-1} [1-F_X(v)]^{n-j} [F_X(v) - F_X(u)]^{j-1-i}}{(i-1)!(j-1-i)!(n-j)!} & -\infty < u < v < \infty \\ 0 & \text{ otherwise } \end{cases}$
\end{thm}

\begin{thm}
    Suppose $X_1, X_2, ..., X_n$ are iid random variables with common pdf $f$ and CDF $F$. Let $g$ be a real valued function such that $E|g(X)| < \infty$ where $X \sim F$. Then for $1 \leq j \leq n$, $E|g(X_{(j)})|$ exists. Converse holds as well.
\end{thm}

\begin{note}
    If $E|g(X_{(j)})| = \infty$ for some $j$, then $E|g(X)| = \infty$ and conversely, \\if $E|g(X)|=\infty$, then $E|g(X_{(j)}| = \infty$ for some $j$.
\end{note}

\begin{defn}
    Suppose a sequence of random variables $(X_n)_{n \geq 1}$ and a random variable $X$ are defined on a probability space $(\Omega, \mathcal{F}, P)$. We say that the sequence of random variables $X_1, X_2, ...$ \textbf{converges in probability} to the random variable $X$ (written $X_n \overset{p}{\to} X$) if for every $\epsilon > 0$, $ \lim_{n \to \infty} P(|X_n - X| > \epsilon) = 0$ or equivalently $\lim_{n \to \infty} P(|X_n - X| \leq \epsilon) = 1$
\end{defn}

\begin{note}
    The random variables in the sequence $X_1, X_2, ...$ are typically not iid random variables
\end{note}

\begin{thm}
    Suppose $X_n \overset{p}{\to} X$ and $Y_n \overset{p}{\to} Y$. Then $X_n \pm Y_n \overset{p}{\to} X \pm Y$  and $X_n Y_n \overset{p}{\to} XY$
\end{thm}

\begin{thm}
    Suppose $X_n \overset{p}{\to} a$, where $a$ is a non-zero constant. Then $\frac{1}{X_n} \overset{p}{\to} \frac{1}{a}$
\end{thm}

\begin{thm}
    Let $X_n \overset{p}{\to} X$ and $h$ be a real valued continuous function of a real variable. Then $h(X_n) \overset{p}{\to} h(X)$
\end{thm}

\begin{defn}
    We say that a sequence of estimators $W_n = W_n(X_1, ..., X_n)$ is a \textbf{consistent sequence of estimators} of the parameter $\theta$ if $W_n \overset{p}{\to} \theta$ as $n \to \infty$ for each fixed $\theta \in \Theta$.
\end{defn}

\begin{note}
    This basically means the estimator converges to the proper value as the sample size becomes infinite, i.e. approaches the size of the population itself.
\end{note}

\begin{thm}[Weak Law of Large Numbers]
    Let $X_1, X_2, ..$ be iid random variables with $EX_i = \theta$. Define $\overline{X_n} = \frac{1}{n} \sum_{i=1}^n X_i$. Then $\overline{X_n} \overset{p}{\to} \theta$. \\
    The weak law of large numbers states that for any population with a finite mean $\theta$, the sample mean $\overline{X_n}$ is a consistent estimator for the population mean $\theta$.
\end{thm}

\begin{thm}[Markov's Inequality]
    Let $X$ be a random variable with finite $r$-moment where $r > 0$. Then for every $\epsilon > 0$, $P(|X| \geq \epsilon) \leq \frac{E|X^r|}{\epsilon^r}$
\end{thm}

\begin{thm}[Chebyshev's Inequality]
    Let $X$ be a random variable with finite mean $\mu$ and finite variance $\sigma^2$. Then for every $\epsilon > 0$, $P(|X-\mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}$
\end{thm}

\begin{thm}
    Suppose we have a sequence $X_1, X_2, ...$ of iid random variables with $EX_i = \mu$ and $Var(X_i) = \sigma^2 < \infty$. Define the sample variance $S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X_n})^2$. Then a sufficient condition that $S_n^2$ converges in probability to $\sigma^2$ is that $Var(S_n^2) \to 0$ as $n \to \infty$.
\end{thm}


\begin{thm}
    Consider a sequence of estimators $W_n$ each having finite mean and variance. If $W_n$ is a sequence of estimators such that $EW_n \to \theta$ and $Var(W_n) \to 0$ as $n \to \infty$, then $W_n$ is consistent for $\theta$.
\end{thm}

\begin{thm}
    Let $W_n$ be a consistent sequence of estimators for a parameter $\theta$. Let $a_1, a_2, ...$ and $b_1, b_2, ...$ be sequences of real numbers such that $a_n \to 1$ and $b_n \to 0$. Then the sequence $U_n = a_n W_n + b_n$ is a consistent sequence of estimators of $\theta$.
\end{thm}

\begin{thm}
    If $S_n^2$ is a consistent estimator of $\sigma^2$, then the sample standard deviation $Sn = \sqrt{S_n^2}$ is a consistent estimator of $\sigma$
\end{thm}

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a random sample. The \textbf{sample moment of order $k$} (where $k$ is a positive integer) is defined as $m_k = \frac{1}{n} \sum_{i=1}^n X_i^k$
\end{defn}

\begin{note}
    Even if the population does not have any moment, sample moments of all orders exists
\end{note}

\begin{defn}
    We say that a sequence of random variables $X_1, X_2, ...$ \textbf{converges in distribution} to a random variable $X$ (written $X_n \overset{d}{\to} X$) if $\lim_{n \to \infty} F_{X_n}(x) = F_X(x)$ at all points $x$ where $F_X(x)$ is continuous.
\end{defn}

\begin{note}
    The convergence of distribution functions does not imply the convergence of corresponding PMFs or PDFs.
\end{note}

\begin{thm}
    Assume that the random variables $X$ and $X_n$ (for each $n$) are non-negative and integer valued. Then $X_n \overset{d}{\to} X \iff \lim_{n \to \infty} f_{X_n} (k) = f_X(k)$ for all $k = 0, 1, 2, ...$
\end{thm}

\begin{thm}
    Let $(X_n)$ and $X$ be random variables with pdf such that $f_{X_n}(x) \to f_X(x)$ for almost all $x \in \mathbb{R}$. 
    Then $X_n \overset{d}{\to} X$.
\end{thm}

\begin{note}
    Convergence in distribution does not have the usual properties associated with convergence. For example, unless $X_n$ and $Y_n$ are independent, then in general $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} Y$ does not imply that $X_n + Y_n \overset{d}{\to} X + Y$
\end{note}

\begin{thm}
    If $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} a$ where $a$ is a constant, then $X_n + Y_n \overset{d}{\to} X + a$ and $X_n Y_n \overset{d}{\to} aX$
\end{thm}

\begin{thm}
    If the sequence of random variables $X_1, X_2, ...$ converges in probability to a random variable $X$, then the sequence also converges in distribution to $X$.
\end{thm}

\begin{note}
    The converse of the above theorem is not true.
\end{note}

\begin{thm}
    Suppose $X_n \overset{d}{\to} a$ where $a$ is a constant. Then $X_n \overset{p}{\to} a$
\end{thm}

\begin{thm}[Central Limit Theorem]
    Let $X_1, X_2, ...$ be a sequence of independent and identically distributed random variables, each having a finite mean $\mu$ and non-zero variance $\sigma^2$. 
    \\Then $\frac{Z_n - n \mu}{\sigma \sqrt{n}} \overset{d}{\to} N(0,1)$ where $Z_n = X_1 + X_2 + ... + X_n$
\end{thm}

\begin{thm}[Continuity Theorem]
    Let $(X_n)$ be a sequence of random variables with corresponding MGFs $(M_n)$ and suppose that $M_n(t)$ exists for $|t| \leq t_0$ for every $n$. If there exists a random variable $X$ with corresponding MGF $M$ which exists for $|t| \leq t_1 \leq t_0$ such that $M_n(t) \to M(t)$ as $n \to \infty$ for every $t \in [-t_1, t_1]$ then $X_n \overset{d}{\to} X$.
\end{thm}

\begin{defn}
    Let $(X_n)$ be a sequence of random variables. We say that $X_n$ is \textbf{asymptotically normal} with mean $\mu$ and variance $\sigma_n^2$ and we write $X_n$ is $AN(\mu_n, \sigma_n^2)$ if $\sigma_n > 0$ and $\frac{X_n - \mu}{\sigma_n} \overset{d}{\to} N(0,1)$ as $n \to \infty$.
\end{defn}

\begin{note}
    Here $\mu_n$ is not necessarily the mean of $X_n$ and $\sigma_n^2$  is not necessarily its variance.
\end{note}

\begin{thm}[Delta Method]
    Suppose $Y_n$ is $AN(\mu, \sigma^2)$ with $\sigma_n \to 0$ and $\mu$ a fixed real number. 
    Let $g$ be a real-valued function which is differentiable at $x = \mu$, with $g'(\mu) \neq 0$. 
    Then $g(Y_n)$ is $AN(g(\mu), [g'(\mu)]^2 \sigma_n^2)$
\end{thm}

\begin{thm}[$k$-th order Delta Method]
    Suppose $Y_n$ is $AN(\mu, \sigma^2)$ with $\sigma_n \to 0$ and $\mu$ a fixed real number. 
    Let $g$ be a real valued function which is differentiable $k$ times, $k \geq 1$ at $x = \mu$ with $g^{(i)}(\mu) = 0$ for $1 \leq i \leq k - 1$, $g^{(k)}(\mu) \neq 0$. Then $\frac{g(Y_n) - g(\mu)}{\frac{1}{k!} g^{(k)}(\mu) \sigma_n^k} \overset{d}{\to} Z^k$ where $Z \sim N(0,1)$
\end{thm}

\hhrule
