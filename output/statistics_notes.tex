\documentclass[12pt,twoside]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,graphicx,amsthm}
\usepackage{epsfig,amssymb}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{lipsum}
\usepackage{thmtools}
\usepackage{enumitem}
\usepackage[top=1.25in, bottom=1.25in,left=1.5in, right=1.25in]{geometry}

\newtheorem{defn}{Definition}
\newtheorem{note}{Note}
\newtheorem{notation}[subsection]{Notation}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}[subsection]{Corollary}
\newtheorem{eg}[subsection]{Example}
\newtheorem{lemma}[subsection]{Lemma}
\newtheorem{result}[subsection]{Result}
\declaretheoremstyle[bodyfont=\normalfont]{normalbody}
\declaretheorem[style=normalbody,name=]{ex}
\newtheorem{remark}[subsection]{Remark}
\newtheorem{prop}[subsection]{Proposition}
\newenvironment*{ans}{\textbf{ans.}\space\em\\}{\par}
\newenvironment*{source}{\hfill\scriptsize\textbf{Source.}\space}{\par}

\declaretheoremstyle[headfont=\normalfont]{normalhead}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\widowpenalties 1 10000
\raggedbottom
\setlength{\parindent}{0pt}
\renewcommand{\chaptername}{}

\setlength{\oddsidemargin}{36pt}
\setlength{\evensidemargin}{36pt}

\begin{document}

\tableofcontents
\newpage
\pagenumbering{arabic}

\chapter{Definitions and Theory}
\line(1,0){360} \\

\section{Fundamentals}

\begin{defn}
    Two random vectors $(X_1, X_2, ..., X_n)$ and $(Y_1, Y_2, ..., Y_n)$ are said to be \textbf{independent} if $F(x_1, x_2, ..., x_m, y_1, y_2, ..., y_n) = F_1 (x_1, x_2, ..., x_m) F_2 (x_1, x_2, ..., x_n)$ for all $(x_1, x_2, ..., x_m, y_1, y_2, ..., y_n) \in \mathbb{R}^{m+n}$ where $F, F_1, F_2$ are the joint CDF's of $(X_1, X_2, ..., X_m, Y_1, Y_2, ..., Y_n)$, $(X_1, X_2, ..., X_m)$ and $(Y_1, Y_2, ..., Y_n)$ respectively.
\end{defn}

\begin{thm}
    Let $X = (X_1, X_2, · · · , X_m)$ and $Y = (Y_1, Y_2, · · · , Y_n)$ be independent random vectors. Then the component $X_j$ of $X(j = 1, 2, · · · , m)$ and the component $Y_k$ of $Y(k = 1, 2, · · · , n)$ are independent random variables. If $h$ and $g$ are Borel-measurable functions, $h(X_1, X_2, · · · , X_m)$ and $g(Y_1, Y_2, · · · , Y_n)$ are independent.
\end{thm}

\begin{thm}
    Let $X$ be a random variable with pdf $f(x)$. Then the pdf of $aX+b$ where $a \neq 0, b \in \mathbb{R}$ is given by $\displaystyle \frac{1}{a} f \left (\frac{x-b}{a} \right)$
\end{thm}

\begin{thm}
    If $X$ and $Y$ are independent continuous random variables with pdfs $f_X(x)$ and $f_Y(y)$, then the pdf of $Z = X+Y$ is $f_Z\left (z\right ) = \displaystyle\int_{- \infty}^\infty f_X \left (w\right ) f_Y \left (z - w\right ) \mathop{\mathrm{d} w}$
\end{thm}


\subsection{Expectation, Variance and Moments}

\begin{defn}
    The \textbf{expectation} of a random variable $X$ having values $x_1, x_2, ..., x_n$ is defined as $E\left (X\right ) = x_1  P\left (X = x_1\right ) + x_2 P\left (x = x_2\right ) + ... + x_n P\left (X = x_n\right ) =  \sum_{j = 1}^n x_j P\left (X = x_j\right ) = x_1 f\left (x_1\right ) + x_2 f\left (x_2\right ) + ... + x_n f\left (x_n\right ) = \sum_{j=1}^n x_j f\left (x_j\right )$ where $f$ is the distribution of $X$.
\end{defn}


\begin{samepage}
\begin{thm}
    The expectation has the following properties:

    \begin{enumerate}
        \item $E(c X) = c E(X)$ where $c$ is any constant
        \item If $X$ and $Y$ are any random variables then $E(X + Y) = E(X) + E(Y)$
        \item If $X$ and $Y$ are independent random variables, then $E(XY) = E(X)E(Y)$
    \end{enumerate}
\end{thm}
\end{samepage}

\begin{defn}
    The \textbf{variance} is defined as $\text{Var} (X) = \sigma ^2 = E[ \  (X - \mu)^2 \  ]$.
    The \textbf{standard deviation} is defined as $\sigma = \sqrt{\text{Var} (X)}$.
\end{defn}

\begin{samepage}
\begin{thm}
    The variance has the following properties:

    \begin{enumerate}
        \item $\sigma ^2 = E\left [ \left (X - \mu \right ) ^2 \right ] = E\left (X ^2\right ) - \mu ^2 = E\left (X ^2\right ) - \left [E\left (X\right )\right ] ^2$ where $\mu = E\left (X\right )$
        \item If $c$ is any constant, $\text{Var}(cX) = c^2 \text{Var}(X)$
        \item The quantity $E[ (X - a)^2 ]$ is a minimum where $a = \mu = E(X)$
        \item If $X$ and $Y$ are independent random variables, then \\ $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)$ and $\text{Var}(X-Y) = \text{Var}(X) + \text{Var}(Y)$
    \end{enumerate}
\end{thm}
\end{samepage}

\begin{defn}
    The $r$-th \textbf{moment} of a random variable $X$ about the mean $\mu$ is defined as $\mu_r = E[\  (X - \mu)^r \ ]$ where $r=0, 1, 2, ...$.

    The $r$-th moment of $X$ about the origin is defined as $\mu_r ' = E(X^r)$.
\end{defn}

It follows that $\mu_0 = 0$, $\mu_1 = 1$, $\mu_2 = \sigma ^2$

\begin{defn}
    Let $X$ be a random variable defined on $\left (\Omega,  \mathcal{F}, P\right )$. The function $M\left (t\right ) = E\left [e ^{t X}\right ]$ is called the \textbf{moment generating function} (MGF) of the random variable $X$ if the expectation on the right side exists in some neighbourhood of the origin. If the expectation on the right side does not exist in any neighbourhood of the origin, then we say the MGF does not exist.
\end{defn}

\begin{samepage}
The $r$-th derivative of the moment generating funtion is the $r$-th moment about the origin $\mu_r '$.
\begin{thm}
    If the MGF $M(s)$ of a random variable $X$ exists, then the MGF $M(s)$ has derivatives of all orders at $s=0$ and \\

    $M^{(k)} (s) |_{s=0} = EX^k$ for positive integer $k$
\end{thm}

\end{samepage}
\begin{thm}
    The moment generating function has the following properties:
    \begin{enumerate}
        \item For any constants $a$ and $b$, the mgf of the random variable $a X + b$ is given by $M_{aX+b} = e^{bt} M_X(at)$
        \item If $X$ and $Y$ are independent random variables having moment generating functions $M_X(t)$ and $M_Y(t)$ respectively, then $M_{X+Y}(t) = M_X(t) M_Y(t)$
        \item \textbf{Uniqueness Theorem} Suppose that $X$ and $Y$ are random variables having moment generating functions $M_X(t)$ and $M_Y(t)$ respectively. Then $X$ and $Y$ have the same probability distribution if and only if $M_X(t) = M_Y(t)$ identically.
    \end{enumerate}
\end{thm}

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a jointly distributed or $(X_1, X_2, ..., X_n)$ be a random vector. If $E[\exp ( \sum_{j=1}^n t_j X_j )]$ exists for $|t_j| \leq h_j$, $j = 1, 2, ...,n$, we write $M(t_1, t_2, ..., t_n) = E[\exp(t_1 X_1 + t_2 X_2 + ... + t_n X_n)]$ and call it the MGF of $X_1, X_2, ..., X_n$ or simply, the \textbf{joint moment generating function} (joint MGF) of the random vector $(X_1, X_2, ..., X_n)$
\end{defn}

\begin{thm}
    The joint MGF $M(t_1, t_2)$ uniquely determines the joint distribution of $(X, Y)$.
    Conversely, if the joint MGF exists it is unique.
\end{thm}

\begin{thm}
    The joint MGF $M(t_1, t_2)$ completely determines the marginal distributions of $X$ and $Y$.

    $M(t_1, 0) = E[\exp(t_1 X)] = M_X (t_1)$ and $M(0, t_2) = E[\exp(t_2 X)] = M_Y(t_2)$
\end{thm}

\begin{thm}
    $X$ and $Y$ are independent random variables if and only if $M(t_1, t_2) = M(t_1, 0) M(0, t_2)$ for all $t_1 \in [-h_1, h_1], t_2 \in [-h_2, h_2]$
\end{thm}

\subsection{Distributions}

\begin{defn}
    Let $p$ be the probability that an event will happen in any single Bernoulli trial (trial with outcomes either success or failure). The probability that an event will happen exactly $x$ times in $n$ trials is given by the \textbf{Binomial Random Variable} with pmf distribution $f(x) = {n \choose k} p^x q^{n-x} = \displaystyle \frac{n!}{x! (n-x)!} p^x q^{n-x}$, where $x = 0, 1, ... , n$.
\end{defn}

The binomial random variable has mean $\mu = np$ and variance $\sigma^2 = npq$.

\begin{defn}
    The \textbf{Poisson Random Variable} has pmf distribution $f(x) = \displaystyle \frac{\lambda^x e^{-\lambda}}{x!}$, $x = 0, 1, 2, ...$ and $\lambda > 0$.
\end{defn}

The Poisson Random Variable has mean $\mu = \lambda$ and variance $\sigma^2 = \lambda$.
\begin{prop}
    The Poisson Random Variable with $\lambda = np$ is the limiting case of the Binomial Distribution. It approximates the Binomial Random variable $\text{Binomial}(n,p)$ when $n$ is large and probability of occurrence of an event $p$ is close to $0$.
\end{prop}

\begin{defn}    
    The \textbf{uniform Random Variable} has the pdf distribution $f(x) = \begin{cases} \frac{1}{b-a} & a \leq x \leq b \\ 0 & \text { otherwise }\end{cases}$.
\end{defn}

The uniform random variable has mean $\mu = \frac{a + b}{2}$ and variance $\sigma^2 = \frac{1}{12} (b-a)^2$.

\begin{defn}
    The \textbf{Normal Random Variable}, also known as the gaussian random variable, has pdf distribution $f(x) = \frac{1}{\sigma \sqrt{2 \pi}}  \exp\left(\displaystyle \frac{-(x-\mu)^2}{2 \sigma^2}\right)$ where $-\infty < x < \infty$ where $\mu$ and $\sigma$ are the mean and standard deviation respectively.
\end{defn}

When $\mu = 0$ and $\sigma = 1$, we get the \textbf{standard normal random variable} with distribution $f(x) = \displaystyle\frac{1}{2\pi} \exp(-\frac{x^2}{2})$.

\begin{prop}
    When $n$ is large and neither $p$ or $q$ is too close to $0$, the binomial random variable $X$ can be approximated by a normal distribution with mean $np$ and standard deviation $\sqrt{npq}$. The approximation is very good when $np, nq > 5$.
\end{prop}

\begin{prop}
    The Poisson Distribution approaches the normal distribution $N(\lambda, \sqrt{\lambda})$ as $\lambda \to \infty$, i.e. the Poisson distribution is asymptotically normal.
\end{prop}

\begin{defn}
    A variable the pdf $f(x) = \displaystyle \frac{1}{\sigma \pi (1 + \left( \frac{x - \mu}{\sigma} \right)^2 )}$, $x \in \mathbb{R}$ where $\sigma > 0, \mu \in \mathbb{R}$ is called a \textbf{Cauchy random variable} with parameter $\mu$ and $\sigma^2$.
    We write $X \sim \mathcal{C}(\mu, \sigma^2)$ for Cauchy random $X$ with pdf.
\end{defn}

The Cauchy random variable centered at $0$ (i.e. $\mu = 0$ is given by the distribution $f(x) = \displaystyle \frac{\sigma}{\pi ( \sigma^2 + x^2}$ where $\sigma > 0, -\infty < x < \infty$.

The mean, variance, higher moments, moment generating function of a Cauchy random variable do not exist.

\begin{defn}
    $\Gamma(\alpha)$ denotes the gamma function which is defined as $\Gamma(\alpha) = \displaystyle\int_0^\infty t ^{\alpha - 1} e ^{-t} \mathop{\mathrm{d} t}$, where $\alpha > 0$. It is a commonly used extension of the factorial function.
\end{defn}

\begin{thm}
    The Gamma function has the following properties:
    \begin{enumerate}
        \item The improper integral converges for all $\alpha > 0$.
        \item $\Gamma(\alpha) > 0$  for all $\alpha > 0$, $\Gamma(\alpha) \to \infty$ as $\alpha \to 0$.
        \item $\Gamma(1) = 1$
        \item $\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)$
        \item For $n \in \mathbb{N}$, $\Gamma(n) = (n-1)!$
        \item $\Gamma(\frac{1}{2}) = \sqrt{\pi}$
    \end{enumerate}
\end{thm}

\begin{defn}
    A random variable with the pdf $f(x) = \displaystyle  \frac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha - 1} e^{-x/\beta}$ where $0 < x < \infty, \alpha > 0, \beta > 0$ is called a \textbf{gamma random variable} $G(\alpha, \beta)$ with parameters $\alpha$ and $\beta$.
\end{defn}
$G(1, \beta) = \text{exp}(\beta)$ with pdf $f(x) = \frac{1}{\beta} e^{-x/\beta}$, $x > 0$.

The gamma distribution has mean $\mu = \alpha \beta$ and variance $\sigma^2 = \alpha \beta^2$.

\begin{defn}
    The \textbf{chi-square distribution} is \\
    $f(x) = \displaystyle \frac{1}{\Gamma(\frac{p}{2}) 2^{\frac{p}{2}}} x^{\frac{p}{2} - 1} e ^{-x/2}$ where $0 < x < \infty$.

    \vspace{0.5cm}
    $\chi_p^2$ denotes a chi-square random variable with $p$ degrees of freedom. It is a special case of the gamma distribution.
\end{defn}

The chi-square distribution is a special case of the gamma distribution with $\alpha = \frac{p}{2}$ and $\beta = 2$.

The mean of the chi-square distribution is given by $\mu = p$ and the variance is given by $\sigma^2 = 2p$.

\begin{prop}
Let $X \sim N(0,1)$. Then $X^2 \sim \chi_1^2$
\end{prop}

\begin{prop}
    Let $X_1, X_2, ..., X_n$ be independent normal random variables with mean $0$ and variance $1$. Then $\chi^2 = X_1^2 + X_2^2 + ... + X_p^2$ is chi-square distributed with $p$ degrees of freedom.
\end{prop}

\section{Sampling Theory}
We can either sample \textit{with replacement} or \textit{without replacement}. A finite population sampled with replacement can be considered infinite. Sampling from a very large finite population can similarly be considered as sampling from an infinite population. 

To properly choose the sample, we can make sure that every member of the population has an equal chance of being in the sample.
Normally, since the sample size is much smaller than the population size, sampling without replacement will give practically the same results as sampling with replacement.

For a sample of size $n$ from a population which we assume has distribution $f(x)$, we can choose members of the population at random, each selection corresponding to a random variable $X_1, X_2, ..., X_n$ with corresponding values $x_1, x_2, ..., x_n$. In case we are assuming sampling without replacement, $X_1, X_2, ..., X_n$ will be independent and identically distributed random variables with probability distribution $f(x)$.

\begin{defn}
Let $X$ be a random variable with a distribution $f$, and let $X_1, X_2, ..., X_n$ be iid random variables with the common distribution $f$.

Then the collection $X_1, X_2, ..., X_n$ is called a \textbf{random sample} of size $n$ from the population $f$.
\end{defn}

Since $X_1, X_2, ..., X_n$ are iid, the joint distribution of the random sample is $f(x_1, x_2, ..., x_n) = f(x_1) f(x_2) ... f(x_n)$.

Any quantity obtained from a sample for the purpose of estimating a population parameter is called a sample
statistic, or briefly statistic. Mathematically, a sample statistic for a sample of size n can be defined as a function of the random variables $X_1, X_2, ..., X_n$ as $T(X_1, X_2, ..., X_n)$. This itself is a random variable whose values can be represented as $T(x_1, x_2, ..., x_n)$.

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a random sample of size $n$ from the population whose distribution is $f\left (x|\theta\right )$ (the distribution $f$ with unknown parameter $\theta$). Let $T\left (x_1, x_2, ...,x_n\right )$ be a real-valued or vector-valued function whose domain includes the range of $\left (X_1, X_2, ..., X_n\right )$. Then the random variable or random vector $Y = T\left (X_1, ..., X_n\right )$ is called a \textbf{statistic} provided that $T$ is not a function of any unknown parameter $\theta$.
\end{defn}

For example consider $X \approx N(\mu, \sigma^2)$ where $\mu$ is known but $\sigma$ is unknown. Then $\frac{\sum_{i=1}^n X_i}{\sigma^2}$ is not a statistic but $\frac{\sum_{i=1}^n X_i}{\mu^2}$ is a statistic.

Two common statistics are the sample mean and sample variance.

\begin{defn}
    The \textbf{sample mean} is the arithmetic average of the values in the random sample. It is denoted by $\bar{X} = \displaystyle \frac{X_1 + X_2 + ... + X_n}{n}$.

    The \textbf{sample variance} is the statistic defined by $S^2 = \displaystyle \frac{\sum_{i=1}^n \left (X_i - \bar{X}\right )^2}{n-1}$

    The \textbf{sample standard deviation} is the statistic defined by $S = \sqrt{S^2}$.
\end{defn}

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a random sample from a population $f\left (x| \theta\right )$. We say that a statistic $T\left (X_1, X_2, ...,X_n\right )$ is an \textbf{unbiased estimator} of the parameter $\theta$ if $E(T) = \theta$ for all possible values of $\theta$.
\end{defn}

\begin{thm}
    Let $X_1, X_2, ..., X_n$ be a random sample from a population with mean $\mu $ and variance $\sigma ^2 < \infty$. Then:
    \begin{enumerate}
        \item $E\left (\bar{X}\right ) = \mu $
        \item $\text{Var}\left (\bar{X}\right ) = \frac{\sigma ^2}{n}$
        \item $E\left (S ^2\right ) = \sigma ^2$
    \end{enumerate}
\end{thm}

From the above theorem we see that the sample mean $\bar{X}$ is an unbiased estimator of the population mean $\mu$ and the sample variance $S^2$ is an unbiased estimator of the population variance $\sigma^2$. (The reason we included $1/n-1$ in the  definition of the sample variance was to make it an unbiased estimator)

\begin{defn}
    Let $X_1, X_2, ..., X_n$ be a random sample of size $n$ from a population $f(x|\theta)$. The probability distribution of a statistic $T(X_1, X_2, ..., X_n)$ is called the sampling distribution of $T$.
\end{defn}

\begin{thm}[Distribution of the sample mean]
    Let $X_1, X_2, ..., X_n$ be a random sample from a population with MGF $M_X(t)$. Then the MGF of the sample mean is $M_{\bar{X}} (t) = (M_X (t/n))^n$.
\end{thm}

\begin{thm}
    Let $X_1, X_2, . . . , X_n$ be a random sample from a $N(\mu, \sigma^2)$ distribution, and let $X$ denote the sample mean.

    Then $X$ and the random vector $(X_1 - X, X_2 - X, . . . , X_n - X)$ are independent.
\end{thm}

\begin{thm}
    Let $X_1, X_2, . . . , X_n$ be a random sample from a $N(\mu, \sigma^2)$ distribution, 
    and let $X$ denote the sample mean and $S^2$ denote the sample variance. Then $X$ and $S^2$ are independent random variables.
\end{thm}
The converse of this theorem is also true: if the sample mean and sample variance of a random sample are independent random variables then population distribution is normal.

\newpage
\chapter{Exercises}
\line(1,0){360} \\


\begin{samepage}
\begin{ex}
    \begin{enumerate}[label=(\roman*)]
        \item[]
        \item Let $X$ be a standard normal variable $N(0,1)$. Find the moment generating function of $X$.
        \item Let $Y$ be a standard normal variable $N(\mu,\sigma ^2)$. Find the moment generating function of $Y$.
    \end{enumerate}
\end{ex}
\begin{ans}
$M_X(t) = e^{\frac{1}{2} t^2}$ \\
$M_Y(t) = e^{\mu t} e^{\frac{\sigma^2 t^2}{2}}$ 
\end{ans}
\begin{source}
Class, Lec 04
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X$ have the PMF $P(X = k) = \frac{6}{\pi ^2 k^2}$ where $k = 1,2, ...$.
Show that the MGF of $X$ does not exist.
\end{ex}
\begin{source}
Class, Lec 04
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X \approx \exp(\lambda)$ with pdf $f(x) = \lambda e^{- \lambda x}$, $x \geq 0$. Then show that $E[e^{tX}]$ exists for all $t < \lambda$.
\end{ex}
\begin{source}
Class, Lec 04
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $Z_1, Z_2, ..., Z_n$ be a random sample from $\mathcal{C}(0,1)$ distribution. Derive the distribution of sample mean $\bar{Z}$.
\end{ex}
\begin{ans}
$\text{Cauchy}(0,1)$
\end{ans}
\begin{source}
Class, Lec 05
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Show that the MGF of the gamma distribution $G(\alpha, \beta)$ is $M_G = \displaystyle \frac{1}{(1 - \beta t)^\alpha}$ for $t < \displaystyle \frac{1}{\beta}$. Hence compute the mean and variance of the gamma distribution.
\end{ex}
\begin{source}
Class, Lec 06
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X_1, X_2, ..., X_n$ be independent random variables such that  $X_j \sim G(\alpha_j, \beta)$, $j = 1, 2, ...,n$.
Then show that $X_1 +  X_2 + ... + X_n \sim G(\sum_{j=1}^n \alpha_j , \beta)$.
\end{ex}
\begin{source}
Class, Lec 06
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X \sim N(0,1)$. Then show that $X^2 \sim \chi_1^2$
\end{ex}
\begin{source}
Class, Lec 06
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X \sim U(0,1)$. Then show that $-2 \ln X \sim \chi^2_2$
\end{ex}
\begin{source}
Class, Lec 06
\end{source}
\end{samepage}

\begin{ex}
Let $X_1, X_2, ..., X_n$ be a random sample of $n$ identical circuit boards whose times until failure are thought to follow an exponential($\beta$) population. Find the joint distribution of the sample. What is the probability that all the boards last more than 2 years?
\end{ex}
\begin{ans}
$f(x_1, x_2, ..., x_n)  = f(x_1) f(x_2) ... f(x_n) = \Pi_{i=1}^n \frac{1}{\beta} \exp(\frac{-x_i}{\beta})$ \\
$P(X_1 > 2, X_2 > 2, ..., X_n > 2) = \exp(\frac{-2n}{\beta})$
\end{ans}
\begin{source}
    Class, Lec 02
\end{source}

\begin{ex}
If sample $X_1, X_2, ..., X_n$ are drawn from  a finite population without replacement, then show that the random variables $X_1, X_2, ...,X_n$ are not mutually independent but that they are identically distributed.
\end{ex}
\begin{source}
Class, Lec 02 
\end{source}

\begin{samepage}
\begin{ex}
Let $X \approx \text{Bernoulli}\left (p\right )$ where $p$ is possibly unknown. Suppose that five independent observations on $X$ are $0,1,1,1,0$ Then find the sample mean, sample variance and sample standard deviation.
\end{ex}
\begin{ans}
Mean $\bar{x} = 0.6$\\
Variance $s^2 = 0.3$ \\
Standard Deviation $s = 0.55$ 
\end{ans}
\begin{source}
Class, Lec 03
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
    Let $X_1, X_2, ..., X_n$ be a random sample from a population with mean $\mu $ and variance $\sigma ^2 < \infty$. Then show that:
    \begin{enumerate}
        \item $E\left (\bar{X}\right ) = \mu $
        \item $\text{Var}\left (\bar{X}\right ) = \frac{\sigma ^2}{n}$
        \item $E\left (S ^2\right ) = \sigma ^2$
    \end{enumerate}
\end{ex}
\begin{source}
Class, Lec 03
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Color blindness appears in 1\%  of the people in a certain population. How large must a sample be if the probability of its containing a color-blind person is to be 0.95 or more? (Assume that the population is large enough to be considered infinite, so that sampling can be considered to be with replacement.)
\end{ex}
\begin{ans}
Sample size greater than 299
\end{ans}
\begin{source}
Class, Lec 03
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Suppose that five observations on normal population are \\
$-0.864, 0.561, 2.355, 0.582, -0.774$.
Compute sample variance.
\end{ex}
\begin{ans}
1.648
\end{ans}
\begin{source}
Class, Lec 03
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
    For a sample of size 5 that results in 7,9,1,6,2, find the sample mean and variance.
\end{ex}
\begin{ans}
Sample Mean $\bar{x} = 6$
Sample Variance $S^2 = 2.5$
\end{ans}
\begin{source}
Schaum's Example 5.5
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
    Let $X_1, X_2, ..., X_n$ be a random sample from a population with MGF $M_X(t)$. Then show that the MGF of the sample mean is $M_{\bar{X}} (t) = (M_X (t/n))^n$.
\end{ex}
\begin{source}
Class, Lec 04
\end{source}
\end{samepage}
\begin{samepage}
    \begin{ex}
        Let $X_1, X_2, ..., X_n$ be a random sample from a $N(\mu, \sigma^2)$ population. Find the distribution of the sample mean.
    \end{ex}
    \begin{ans}
        $\exp({\mu t + \frac{(\sigma^2 / n)t^2}{2}})$, i.e. $\bar{X}$ has a $N(\mu, \frac{\sigma^2}{n})$ distribution.
    \end{ans}
    \begin{source}
        Class, Lec 04
    \end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X \sim \mathcal(C)(0,1)$. Show that:
    \begin{enumerate}
        \item $EX$ does not exist 
        \item the mean does not exist
        \item the MGF does not exist
    \end{enumerate}
\end{ex}
\begin{source}
Class, Lec 04
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X_1, ..., X_n$ be iid random variables with continuous cdf $F_X$ and suppose $EX_i = \mu$. Define the random variables $Y_1, Y_2, ..., Y_n$ by $Y_i = \begin{cases} 1 & \text{if } X_i > \mu \\ 0 & \text{if } X_i \leq \mu \end{cases}$.
Find the distribution of $\sum_{i=1}^n Y_i$
\end{ex}
\begin{ans}
$\text{Binomial}(n,1-F_X (\mu))$
\end{ans}
\begin{source}
Class, Lec 04
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
A fair die is rolled. Let $X$ be the face value that turns up, and $X_1, X_2$ be two independent observations on $X$. Compute the PMF of $\bar{X}$.
\end{ex}
\begin{source}
Class, Lec 04
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X_1, X_2, ..., X_n$ be independent observations on Poisson population with parameter $\lambda$. Find the distribution of the sample mean.
\end{ex}
\begin{ans}
$P(\bar{X} = t) = \displaystyle \frac{e^{-n \lambda} (n \lambda)^{tn}}{(tn)!}$ where $t = 0, \frac{1}{n}, \frac{2}{n}, ...$
\end{ans}
\begin{source}
Class, Lec 04
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X_1, X_2, . . . , X_n$ be a random sample from a $N(\mu, \sigma^2)$ distribution, and let $X$ denote the sample mean.

Then show that $X$ and the random vector $(X_1 - X, X_2 - X, . . . , X_n - X)$ are independent.
\end{ex}
\begin{source}
Class, Lec 06
\end{source}
\end{samepage}

\begin{samepage}
\begin{ex}
Let $X_1, X_2, . . . , X_n$ be a random sample from a $N(\mu, \sigma^2)$ distribution, and let $X$ denote the sample mean and $S^2$ denote the sample variance. Then show that $X$ and $S^2$ are independent random variables.
\end{ex}
\begin{source}
Class, Lec 06
\end{source}
\end{samepage}

\end{document}

